{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "PSY 394U <b>Data Analytics with Python</b>, Spring 2018\n",
    "\n",
    "\n",
    "<img style=\"width: 400px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Title_pics.png?raw=true\" alt=\"title pics\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:40px; margin-bottom: 30px;\"><b> Logistic regression </b></p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:18px; margin-bottom: 32px;\"><b>February 13, 2018</b></p>\n",
    "\n",
    "<hr style=\"height:5px;border:none\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is logistic regression?\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "In a typical linear regression, the dependent variable $Y$ is expressed as a linear combination of independent variables $X_1$, $X_2$, ... $X_p$\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "Here, $\\beta$s are unknown regression coefficients to be estimated, and $\\epsilon$ is the error assumed to follow a normal distribution. This model works well if $Y$ is a continuous variable. But say if $Y$ is a binary variable, taking values either 0 or 1. In that case, the linear model above does not work well since Y is not exactly 0 or 1. \n",
    "\n",
    "The solution to modeling a binary outcome variable is to use a logistic regression. Rather than modeling $Y$ directly, it models the probability that $Y$ is 1, or $Pr(Y=1)$. A logistic regression model has the following form:\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$p = \\frac{1}{1+\\exp\\left(-\\left(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \n",
    "                             + \\cdots + \\beta_p X_p\\right)\\right)}$$\n",
    "\n",
    "where $p=Pr(Y=1)$. The function $\\log\\left(\\frac{p}{1-p}\\right)$ is referred as a **logit** function. It converts a probability (between 0 and 1) into a real number (between $-\\infty$ and $\\infty$). With this model, the probability $p=Pr(Y=1)$ can be modeled as a function of independent variables $X$s. Here is an example of the probability $p=Pr(Y=1)$ as it relates to an independent variable $X$ in a logistic regression.\n",
    "\n",
    "<img style=\"width: 500px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Logistic_LogitFunc.png?raw=true\" alt=\"Logistic function\"/>\n",
    "\n",
    "Once a logistic regression model is learned from the data (i.e., all parameters are estimates), then we can use the model as a classifier. For example, if the predicted probability $p>0.5$ then one can predict *success* (i.e., $Y=1$), whereas one can conclude *failure* (i.e., $Y=0$) when $p<0.5$.\n",
    "\n",
    "Just a side note, the quantity $\\frac{p}{1-p}$ is often referred as *odds*. This is different from *probability*. The probability of rolling 6 on a balanced die 1/6, but the odds of rolling 6 is 1/5. This is because odds is the ratio of the probability of getting 6 to the probability of not getting 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example: breast cancer data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "You may recall the breast cancer data from the [LDA lecture](https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/LinDisc.ipynb). The data set (see **`WiscBrCa_clean.csv`**) includes the diagnosis information (benign or malignant) on N=683 breast tumors along with some pathology features on the tumors. The goal here is to build a logistic regression classifier to predict whether a tumor is malignant based on the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<WiscBrCaLogistic.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# loading the data, creating binary target\n",
    "BrCaData = pd.read_csv('WiscBrCa_clean.csv')\n",
    "BrCaData['Malignant'] = BrCaData.Class //2 -1  # Binary variable of malignancy\n",
    "                                               # 0: benign\n",
    "                                               # 1: malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if there is any difference between benign and malignant tumors, by calculating feature means for these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ID  ClumpThick  UniCellSize  UniCellShape  Adhesion  \\\n",
      "Malignant                                                                  \n",
      "0          1.115261e+06    2.963964     1.306306      1.414414  1.346847   \n",
      "1          1.005121e+06    7.188285     6.577406      6.560669  5.585774   \n",
      "\n",
      "           EpiCellSize   BareNuc  Chromatin  Nucleoli   Mitoses  Class  \n",
      "Malignant                                                               \n",
      "0             2.108108  1.346847   2.083333  1.261261  1.065315    2.0  \n",
      "1             5.326360  7.627615   5.974895  5.857741  2.602510    4.0  \n"
     ]
    }
   ],
   "source": [
    "# means according to malignancy\n",
    "print(BrCaData.groupby('Malignant').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the means are substantially different in some of the features.\n",
    "\n",
    "Now let's prepare the data for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data for logistic regression\n",
    "BrCaFeatures = np.array(BrCaData.iloc[:,1:10])\n",
    "BrCaTargets = np.array(BrCaData.Malignant)\n",
    "featureNames = np.array(BrCaData.columns[1:10])\n",
    "targetNames = ['Benign','Malignant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And splitting the data into the training and testing data sets. The testing data contains 33% of observations. Here, I set **`random_state=0`** so that I can (and you can too!) replicate the results later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting the data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(BrCaFeatures, BrCaTargets,\n",
    "                                                    test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fitting the logistic regression model. We will use **`LogisticRegression`** object under **`sklearn.linear_model`**. Once we define a logistic regression object, then we can use the **`.fit`** method to learn from the data (*feature* and *target* info, as in LDA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the logistic regression to the training data\n",
    "BrCaLR = LogisticRegression()\n",
    "BrCaLR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model is fitted to the data. We can examine the resulting regression coefficients (i.e., $\\beta$s) to see which feature is contributing to outcome. Features with a large magnitude (either positive or negative) are highly associated with $Pr(Y=1)$ (in this case, a malignant tumor). \n",
    "\n",
    "If you have learned logistic regression before, you may know that, if you exponentiate a regression coefficient $\\beta$, you get an odds ratio associated with a unit increase of the feature. So, let's take a look at the odds ratios rather than regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature    \tOdds Ratio\n",
      "ClumpThick  \t   1.134\n",
      "UniCellSize \t   1.343\n",
      "UniCellShape\t   1.102\n",
      "Adhesion    \t   1.127\n",
      "EpiCellSize \t   1.005\n",
      "BareNuc     \t   1.546\n",
      "Chromatin   \t   1.164\n",
      "Nucleoli    \t   1.327\n",
      "Mitoses     \t   1.041\n"
     ]
    }
   ],
   "source": [
    "# Printing out the odds ratios\n",
    "print('Feature    \\tOdds Ratio')\n",
    "for i,iFeature in enumerate(featureNames):\n",
    "    print('%-12s' % iFeature, end='')\n",
    "    # Odds ratio associated with each feature (unit increase)\n",
    "    print('\\t%8.3f' % np.exp(BrCaLR.coef_[0,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a unit increase in **`BareNuc`** increases the odds of a malignant tumor 1.5 times. If the regression coefficient $\\beta$ is positive, that means the effect is associated with a higher odds (thus likelihood) of $Y=1$. On the other hand, a negative $\\beta$ is associated with a lower odds of $Y=1$.\n",
    "\n",
    "Just for fun, let's plot the predicted probability of $Y=1$ as well as the predicted classification ($Y=0$ or $1$). Here, we can get the predicted classification with the **`.predict`** method associated with the logistic regression object **`BrCaLR`**. we can get the predicted probability of $Y=1$ with the **`.predict_proba`** method. Both methods only require a testing data set as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAFyCAYAAADMJ2F9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XuYJGV59/HvDSyHhWXVoMAlAgIKGBCYkZOLQcGIRzyL\nk5ioqAlKoqwRXyTmRUFijApilEg0chAZ9TWEgCgoaCTKIsmsgMtJVJSDuGDABWE57v3+UTXQ2/TM\nTtf2PNPT8/1cV13TXfVUPXc/szv96zp1ZCaSJEklrTPTBUiSpLnHACJJkoozgEiSpOIMIJIkqTgD\niCRJKs4AIkmSijOASJKk4uZMAImI+RExFBHzZ7oWSZJmk+l4D12vVxuaBXYHfgj8aURcN9PFSJI0\ni+wEfBlYBFzaiw3OpQCybf3zyzNZhCRJs9i2GEC69kuAM888k5133nmGS5k7Fi9ezIknnjjTZcwp\njnl5jnl5jnlZ1157LW9605ugfi/thbkUQO4H2HnnnRkaGprpWuaMhQsXOt6FOeblOeblOeYz5v5e\nbWjOnIQqSZL6hwFEkiQVZwCRJEnFGUA0rUZGRma6hDnHMS/PMS/PMZ/9IjNnuoYiImIIGBsbG/PE\nJUmSurB06VKGh4cBhjNzaS+22Rd7QCLieRFxbkTcGhGrIuLgKazz/IgYi4j7I+KnEfHmErVKkqS1\n1xcBBNgYuAI4HFjjLpmI2Bb4BnAxsBtwEvCFiPjj6StRkiT1Sl/cByQzLwAuAIiImMIq7wR+kZnv\nr59fHxH7AYuB70xPlZIkqVf6ZQ9It/YBLmqbdyGw7wzUIkmSujRbA8gWwPK2ecuBTSNigxmoR1JB\ny5fDfvvB9ttXP2+/3Vpmwvhr33Zb2HTT6mepMWg67uPrbbPN6jUvW1b299he/7JlsPfesOGG1bTX\nXlUNy5ZVdc6bV/28+uqJtzc0BOusAxGw8caw666wwQbVvPXWq+Y3narzT3ssM/tqAlYBB6+hzfXA\n/2mb91LgEWD9CdYZAnJsbCwlzW6LFmXCY9OiRdYyE9pfe8kxaDruE9W8YEHZ19BeR3v/4zW0z1+w\noLvX1btpLKnO0RzKHr3f98U5IA38Bti8bd5TgLsz88HJVly8eDELFy5cbd7IyIjXlEuzyG23Tf68\npH6qpbSJXmuJMWg67hO1W7my2faaat9+e//jbdrnd2rXaXtrZ7SeWq3oZQdAn5yE2sAS4CVt815U\nz5/UiSee6H1ApFluyy3hF79Y/bm1lNf+2lvnl+57qn1OVPNGG8E993S/vaba62jvf7zNHXesPn+j\njaa2vbUzUk+tlgK9PQ7TF+eARMTGEbFbROxez9qufv60evlHI+L0llU+B2wfER+LiB0j4l3A64AT\nCpcuaQacfTYsWgTbbVf9PPtsa5kJ4699m21gwYLqZ6kxaDruE9W8ZEnZ32N7/UuWVOd9bLBBNe25\nZ9VmyZKqzvXWq34umeBj9tlnwx57VOdrAMyfD7vsAuuvX81bd93pfT1N9MWdUCNif+B7PP4eIKdn\n5qERcSqwTWYe0LbOCcCzgFuAYzPzS5P04Z1QJUlqYDruhNoXh2Ay8/tMsjcmM986wTrTcV6uJEma\nZn1xCEaSJM0tBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQ\nSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcA\nkSRJxRlAJElScQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEG\nEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVn\nAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScX0TQCLi8Ii4MSJWRsRlEbHnGtofERHX\nRcR9EXFTRJwQERuUqleSJDXXFwEkIg4BPgkcA+wBXAlcGBGbTdD+T4CP1u13Ag4FDgGOL1KwJEla\nK30RQIDFwCmZeUZmXgccBtxHFSw62Rf4QWZ+NTNvysyLgFFgrzLlSpKktTHjASQi5gHDwMXj8zIz\ngYuogkYnlwLD44dpImI74KXA+dNbrSRJ6oX1ZroAYDNgXWB52/zlwI6dVsjM0frwzA8iIur1P5eZ\nH5vWSiVJUk/M+B6QSQSQHRdEPB84mupQzR7Aa4CXR8QHi1UnSZIa64c9IL8FHgE2b5v/FB6/V2Tc\nscAZmXlq/fzqiNgEOAX4yGSdLV68mIULF642b2RkhJGRkW7rliRp4IyOjjI6OrravBUrVvS8nxkP\nIJn5UESMAQcC5wLUh1UOBD49wWrzgVVt81bVq0Z9DklHJ554IkNDQ2tfuCRJA6jTh/KlS5cyPDzc\n035mPIDUTgBOr4PI5VRXxcwHTgOIiDOAWzLz6Lr9ecDiiLgC+BHwDKq9Iv8xWfiQJEn9oS8CSGZ+\nrT6p9FiqQzFXAAdl5h11k62Ah1tWOY5qj8dxwFOBO6j2nngOiCRJs0BfBBCAzDwZOHmCZQe0PR8P\nH8cVKE2SJPVYP18FI0mSBpQBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJx\nBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQV\nZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElS\ncQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVFyjABIR\nm0fElyLi1xHxcEQ80jr1ukhJkjRY1mu43mnA1sBxwG1A9qogSZI0+JoGkP2A52XmFb0sRpIkzQ1N\nzwG5GYheFiJJkuaOpgHkCOAfImLb3pUiSZLmiqaHYL4KzAd+HhH3AQ+1LszMJ61tYZIkaXA1DSBH\n9LQKSZI0pzQKIJl5eq8LkSRJc0fTPSBExLrAq4CdqS7DvQY4NzO9D4gkSZpU0xuR7QBcC5wBvAZ4\nHXAmcHVEbN9wm4dHxI0RsTIiLouIPdfQfmFEfLa+GdrKiLguIl7cpG9JklRW06tgPg38HHhaZg5l\n5h5UNya7sV7WlYg4BPgkcAywB3AlcGFEbDZB+3nARXWfrwF2BN4B3Nr9S5EkSaU1PQSzP7BPZt45\nPiMz/zcijgJ+2GB7i4FTMvMMgIg4DHgZcCjwjx3avw14Ql3D+CGfmxr0K0mSZkDTPSAPAAs6zN8E\neLCbDdV7M4aBi8fnZWZS7eHYd4LVXgEsAU6OiN9ExE8i4gMR4ZfrSZI0CzR9w/4G8C8RsXc8Zh/g\nc8C5XW5rM2BdYHnb/OXAFhOssx3weqr6X0L1nTR/AxzdZd+SJGkGND0E827gdKq9EOM3IVuPKny8\npwd1QXWr94m+5G4dqoDyF/Xekh9HxFOB9wEfmWyjixcvZuHChavNGxkZYWRkZO0rliRplhsdHWV0\ndHS1eStWrOh5P1G9fzdcOeIZwE5UYeGazPxZg23MA+4DXpuZ57bMPw1YmJmv7rDOfwIPZuaLWua9\nGDgf2CAzH+6wzhAwNjY2xtDQULdlSpI0Zy1dupTh4WGA4cxc2ottrtU5E5l5Q2ael5nnNgkf9TYe\nAsaAA8fnRUTUzy+dYLUfAju0zdsRuK1T+JAkSf1lyodgIuIE4O8y89768YQy871d1nECcHpEjAGX\nU10VMx84re77DOCWzBw/x+Ofgb+KiJOAzwDPBD4AfKrLfiVJ0gzo5hyQPYB5LY97JjO/Vt/z41hg\nc+AK4KDMvKNushXwcEv7WyLiRcCJVPcMubV+3OmSXUmS1GemHEAy8wWdHvdKZp4MnDzBsgM6zPsR\n8Nxe1yFJkqZf01uxfzEiHncfkIjYOCK+uPZlSZKkQdb0JNQ3Axt1mL8R8OfNy5EkSXNBV/cBiYhN\nqS65DWBBRNzfsnhd4KXA7b0rT5IkDaJub0T2O6qbgyXw0w7Lk+oL5SRJkibUbQB5AdXej+8CrwXu\nbFn2IPCrzPx1j2qTJEkDqqsAkpnfB4iIpwM3Z+aqaalKkiQNtEbfBZOZvwKIiPnA1sD6bcuvWvvS\nJEnSoGoUQCLiycCpVN9E28m6jSuSJEkDr+lluJ8CngDsDawEXkx1ae4NwMG9KU2SJA2qRntAgAOA\nV2bm/0TEKqqTT78TEXdTfSfL+T2rUJIkDZyme0A25rH7fdwFPLl+/BPA77qXJEmTahpArgd2rB9f\nCfxlRDwVOAy4rReFSZKkwdX0EMyngC3rxx8GLgD+lOpeIG9Z+7IkSdIga3oZ7pdbHo9FxDbATsBN\nmfnbXhUnSZIGU9M9IKvJzPuApb3YliRJGnyNzgGJiK9HxFEd5h8ZEf9v7cuSJEmDrOlJqPvT+VLb\nC4A/al6OJEmaC5oGkE2oTjht9xCwafNyJEnSXNA0gPwEOKTD/DcC1zQvR5IkzQVNT0I9Djg7IrYH\nvlvPOxAYAV7fi8IkSdLganoZ7nkR8SrgaOB1VN8HcxXwwsz8fg/rkyRJA6jxZbiZeT5+54skSWqg\n6TkgkiRJjU15D0hE3Ak8MzN/GxF3ATlR28x8Ui+KkyRJg6mbQzCLgXvqx0dMQy2SJGmO6CaA7AZ8\nHXgAuBG4NDMfnpaqJEnSQOvmHJC/proBGcD3AA+zSJKkRrrZA/JL4N0R8W0ggH3rc0EeJzMv6UFt\nkiRpQHUTQI4EPgd8gOoE1H+foF0C665lXZIkaYBNOYBk5jnAORGxCXA3sCNw+3QVJkmSBlfXNyLL\nzN9HxAuAGz0JVZIkNdHNfUA2zcy766c/BuZHRMe2Le0kSZIep5s9IHdFxJaZeTvwOzrfiCzwHBBJ\nkrQG3QSQA4A768cvmIZaJEnSHNHNSajf7/RYkiSpW42+jC4iXhwR+7U8PzwiroiIsyLiib0rT5Ik\nDaKm34b7cWBTgIjYFTgB+Cbw9PqxJEnShLq+DLf2dOCa+vFrgfMy8+iIGKIKIpIkSRNqugfkQWB+\n/fiFwLfrx3dS7xmRJEmaSNM9ID8AToiIHwJ7AYfU858J3NKLwiRJ0uBqugfkr4CHgdcB78zMW+v5\nLwEu6EVhkiRpcDXaA5KZNwEv7zB/8VpXJEmSBl7Ty3CH6qtfxp+/MiLOiYi/j4j1e1eeJEkaRE0P\nwZxCdb4HEbEd8BXgPuD1wD/2pjRJkjSomgaQZwJX1I9fD1ySmX8CvIXqslxJkqQJNQ0g0bLuC3ns\n3h83A5s12mB1N9UbI2JlRFwWEXtOcb03RsSqiDi7Sb+SJKm8pgHkf4APRsSfAfsD59fznw4s73Zj\nEXEI8EngGGAP4ErgwoiYNMxExDZUd2W9pNs+JUnSzGkaQI4AhoDPAMdn5s/q+a8DLm2wvcXAKZl5\nRmZeBxxGdU7JoROtEBHrAGcC/xe4sUGfkiRphjS9DPcqYNcOi44EHulmWxExDxgG/r5l+xkRFwH7\nTrLqMcDtmXlqRPxRN31KkqSZ1fROqB1l5v0NVtsMWJfHH7pZDuzYaYWIWAS8FditQX+SJGmGNQog\nEbEu1WGTNwBbA6vd+yMzn7T2pRFAduh7E+BLwDsy864e9CNJkgprugfkGODtVCeOfgQ4HtgWeBVw\nbJfb+i3VYZvN2+Y/hc4ntG4PbAOcFxFRz1sHICIeBHbMzAnPCVm8eDELFy5cbd7IyAgjIyNdli1J\n0uAZHR1ldHR0tXkrVqzoeT+R+bidDGteKeLnwLsz8/yIuAfYPTN/HhHvBvap7wnSzfYuA36Ume+p\nnwdwE/DpzPx4W9v1gR3aNnE8sAnwbuCGzHy4Qx9DwNjY2BhDQ0PdlCdJ0py2dOlShoeHAYYzc2kv\nttl0D8gWwE/qx78HxncpfAM4rsH2TgBOj4gx4HKqwzvzgdMAIuIM4JbMPDozHwSuaV05In5Hde7q\ntQ36liRJhTUNILcAW1Ltpfg58CJgKbAn8EC3G8vMr9X3/DiW6lDMFcBBmXlH3WQrqm/flSRJA6Bp\nAPl34EDgR8A/AWdGxNuoTkg9sckGM/Nk4OQJlh2whnXf2qRPSZI0M5reB+SolsdfjYibqO7ZcUNm\nnter4iRJ0mDqyX1AMnMJsKQX25IkSYNvygEkIg6eatvMPLdZOZIkaS7oZg/IOVNsl1R3NpUkSepo\nygEkM5t+cZ0kSdJqugoVEXFARFwTEZt2WLYwIq6OiOf1rjxJkjSIut2rcQTw+cy8u31BZq4ATgHe\n24vCJEnS4Oo2gOwGXDDJ8m8Dw83LkSRJc0G3AWRz4KFJlj8MPLl5OZIkaS7oNoDcCuw6yfJnA7c1\nL0eSJM0F3QaQbwLHRsSG7QsiYiPgw1RfSCdJkjShbu+E+hHgNcBPI+IzwPVU9/3YGTic6v4fx/e0\nQkmSNHC6CiCZuTwingv8M/BRIMYXARcC78rM5b0tUZIkDZquvwsmM38FvDQingjsQBVCbsjMu3pd\nnCRJGkyNv4yuDhz/3cNaJEnSHOHt1SVJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFE\nkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlA\nJElScQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwB\nRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQV1zcBJCIOj4gbI2JlRFwWEXtO0vbtEXFJRNxZT9+Z\nrL0kSeovfRFAIuIQ4JPAMcAewJXAhRGx2QSr7A+cBTwf2Ae4Gfh2RGw5/dVKkqS11RcBBFgMnJKZ\nZ2TmdcBhwH3AoZ0aZ+afZebnMvOqzPwp8Haq13JgsYolSVJjMx5AImIeMAxcPD4vMxO4CNh3ipvZ\nGJgH3NnzAiVJUs/NeAABNgPWBZa3zV8ObDHFbXwMuJUqtEiSpD633kwXMIkAco2NIo4C3gDsn5kP\nrqn94sWLWbhw4WrzRkZGGBkZaVqnJEkDY3R0lNHR0dXmrVixouf9RHW0Y+bUh2DuA16bmee2zD8N\nWJiZr55k3fcBRwMHZuaP19DPEDA2NjbG0NBQT2qXJGkuWLp0KcPDwwDDmbm0F9uc8UMwmfkQMEbL\nCaQREfXzSydaLyKOBP4WOGhN4UOSJPWXfjkEcwJwekSMAZdTXRUzHzgNICLOAG7JzKPr5+8HjgVG\ngJsiYvN6O7/PzHsL1y5JkrrUFwEkM79W3/PjWGBz4AqqPRt31E22Ah5uWeWdVFe9fL1tUx+utyFJ\nkvpYXwQQgMw8GTh5gmUHtD1/epGiJEnStJjxc0AkSdLcYwCRJEnFGUAkSVJxBhBJklScAUSSJBVn\nAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJx\nBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElScQYQSZJUnAFEkiQV\nZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIkFWcAkSRJxRlAJElS\ncQYQSZJUnAFEkiQVZwCRJEnFGUAkSVJxBhBJklScAUSSJBVnAJEkScUZQCRJUnEGEEmSVJwBRJIk\nFdc3ASQiDo+IGyNiZURcFhF7rqH96yPi2rr9lRHxklK1SpKktdMXASQiDgE+CRwD7AFcCVwYEZtN\n0H5f4Czg88DuwDnAORHxrDIVS5KktdEXAQRYDJySmWdk5nXAYcB9wKETtH8P8K3MPCEzr8/MY4Cl\nwF+VKVeSJK2NGQ8gETEPGAYuHp+XmQlcBOw7wWr71stbXThJ+wm96lUQsfbTrrvC7bdX21y+HHbZ\n5fHLly2D/faD7beHvfeGPfaADTZ4rM2CBXD11VOre/ly2Gsv2HDDatp778f6n8q643Xst9/U1muy\nTinttS1b1nlspus1LF9e9THe3157lR/TZctg001h3rzq59VX99fvbLJa+qlOSQVl5oxOwJbAKmDv\ntvkfA5ZMsM4DwCFt894J3DZJP0NAjo2NZSvo3bRoUbXNRYs6L1+wYM3bWLAgp6RTH+P9d7vuVNZr\nsk4p7bV1GudFi6bvNTT9XfSynvbXvGBBf/3OJquln+qU1NnY2FgCCQxlj97/1ysdeLoQVC+2p+0X\nL17MwoUL2+aO1NPaue221X+2W7lyzduYSpuJ+pio3zW1m8p6TdYppb2WTmO4NuPVbf9T3XYvx7T9\nNa9c2V+/s8lq6ac6JcHo6Cijo6OrzVuxYkXP++mHAPJb4BFg87b5TwGWT7DOb7ps/6gTTzyRoaGh\nR59HTLnONdpyy8d+/uIXj1++0UZwzz2Tb2OjjabeV3sf4/13u+5U1muyTinttXUa5/F6p+M1NP1d\n9HJM21/zRhv11+9sslr6qU5JMDIywsjI6h/Kly5dyvDwcG876tWulLWZgMuAk1qeB3AzcOQE7b8C\n/EfbvB8CJ0/SR8dDMK997dQOr6xp2mWXzOXLq20uX575h3/4+OXLllW7l7fbLnOvvTJ33z1z/fUf\na7PJJlWbqVi+PHPPPTM32KCa9trrsf6nsu54HYsWTW29JuuU0l7bsmWdx2a6XsPy5VUf4/3tuWf5\nMV22rDrsst561c9ly/rrdzZZLf1Up6TOpuMQTGR2c5RjekTEG4DTgb8ELqe6KuZ1wE6ZeUdEnAHc\nkplH1+33Bb4PHAWcT3X85Ciqgblmgj6GgLGxsbHV9oBIkqTJtewBGc7Mpb3YZj8cgiEzv1bf8+NY\nqkMrVwAHZeYddZOtgIdb2i+JiBHg+Hq6AXjlROFDkiT1l74IIACZeTJw8gTLDugw79+Af5vuuiRJ\nUu/N+H1AJEnS3GMAkSRJxRlAJElScQYQTav2m9lo+jnm5Tnm5Tnms58BRNPKPxLlOeblOeblOeaz\nnwFEkiQVZwCRJEnFGUAkSVJxfXMjsgI2BLj22mtnuo45ZcWKFSxd2pO79mqKHPPyHPPyHPOyWt47\nN+zVNvviu2BKiIjnUn1hnSRJamZRZl7aiw3NpQAyH9hppuuQJGkWuy4z7+vFhuZMAJEkSf3Dk1Al\nSVJxBhBJklScAUSSJBVnAJEkScUNTACJiMMj4saIWBkRl0XEnmto//qIuLZuf2VEvKRUrYOimzGP\niLdHxCURcWc9fWdNvyM9Xrf/zlvWe2NErIqIs6e7xkHT4G/Lwoj4bET8ul7nuoh4cal6B0GDMT+i\nHuf7IuKmiDghIjYoVe9sFxHPi4hzI+LW+u/EwVNY5/kRMRYR90fETyPizd32OxABJCIOAT4JHAPs\nAVwJXBgRm03Qfl/gLODzwO7AOcA5EfGsMhXPft2OObA/1Zg/H9gHuBn4dkRsOf3VDoYGYz6+3jbA\nx4FLpr3IAdPgb8s84CJga+A1wI7AO4BbixQ8ABqM+Z8AH63b7wQcChwCHF+k4MGwMXAFcDiwxktj\nI2Jb4BvAxcBuwEnAFyLij7vqNTNn/QRcBpzU8jyAW4D3T9D+K8C5bfOWACfP9GuZLVO3Y95h/XWA\nFcCbZvq1zJapyZjX4/xfwFuBU4GzZ/p1zKapwd+Ww4AbgHVnuvbZOjUY838CvtM27xPAJTP9Wmbj\nBKwCDl5Dm48BV7XNGwW+2U1fs34PSP2JY5gqiQGQ1WhcBOw7wWr71stbXThJe7VoOObtNgbmAXf2\nvMABtBZjfgxwe2aeOr0VDp6GY/4K6g8zEfGbiPhJRHwgImb939oSGo75pcDw+GGaiNgOeClw/vRW\nO6ftQw/eQwfhu2A2A9YFlrfNX061+7OTLSZov0VvSxtYTca83ceodku3/yNWZ12PeUQsotrzsdv0\nljawmvw73w44ADgTeAnwDODkejsfmZ4yB0rXY56Zo/XhmR9ERNTrfy4zPzatlc5tE72HbhoRG2Tm\nA1PZyCAEkIkEUziWtRbt9XhTGsOIOAp4A7B/Zj447VUNto5jHhGbAF8C3pGZdxWvarBN9u98Hao/\nxH9Rf3L/cUQ8FXgfBpC1MeGYR8TzgaOpDn9dDuwAfDoibstMx7ycqH9O+X10EALIb4FHgM3b5j+F\nxye0cb/psr1W12TMAYiI9wHvBw7MzKunp7yB1O2Ybw9sA5xXfyqE+qTziHgQ2DEzb5ymWgdFk3/n\ntwEP1uFj3LXAFhGxXmY+3PsyB0qTMT8WOKPlMOPVdQA/BUPfdJnoPfTubj5Uzvrjkpn5EDAGHDg+\nr/6DeyDVscFOlrS2r/1xPV9r0HDMiYgjgb8FDsrMH093nYOkwZhfC+xKdZXXbvV0LvDd+vHN01zy\nrNfw3/kPqT6Bt9oRuM3wsWYNx3w+1YmTrVbVq0aH9lp7nd5DX0S376EzfcZtj87afQOwEvhzqsuw\nTgH+F3hyvfwM4O9b2u8LPAi8l+qPw4eA+4FnzfRrmS1TgzF/fz3Gr6ZKzuPTxjP9WmbL1O2Yd1jf\nq2CmecyBraiu7jqJ6vyPl1F9Wjxqpl/LbJkajPkxwO+oLr3dlurD5A3AWTP9WmbLRHVRwG5UH1hW\nAUfUz59WL/8ocHpL+22B31Ody7cj8K76PfWF3fQ7CIdgyMyv1SchHUv1pnYF1afsO+omWwEPt7Rf\nEhEjVNeJH0/1j/WVmXlN2cpnr27HHHgn1VUvX2/b1IfrbWgNGoy51lKDvy23RMSLgBOp7l9xa/34\nH4sWPos1+Hd+HNWb5nHAU4E7qPb2fbBY0bPfc4DvUZ2/kVT3YQE4neq+KlsATxtvnJm/jIiXAScA\n76a6TPptmdnVRQVRpxlJkqRiZv05IJIkafYxgEiSpOIMIJIkqTgDiCRJKs4AIkmSijOASJKk4gwg\nkiSpOAOIJEkqzgAiDaCI2CYiVkXEs+vn+0fEIxGx6QzU8r2IOKF0v3Xfb46Itf424Ii4MSLevYY2\nqyLi4Pq1qzadAAAHC0lEQVRx34y/1K8MIFIhEXFq/ab0SEQ8EBE3RMQHI2K6/h+23ub4h8CWmXn3\nVFacydAwDUrd7nkL4FsT9Lva+PcqGEmz2UB8F4w0i3wLeAuwIfAS4GTgIaovdVpNHUwym39fwqPf\nBJrVN7He3nA7fSci5mX1zal9IzPbx3ey8Q/KBSOpL7kHRCrrgcy8IzNvzsx/AS4GxnfbvyUi7oqI\nV0TE1VTfHvy0etnbI+KaiFhZ/3xn60YjYq+IWFovvxzYg5Y3uPoQwKrWQwARsaje03FvRNwZEd+K\niIURcSqwP/Celj02W9fr7BIR34yIeyLiNxFxRkT8Qcs259fz7omIWyPivWsakIg4JiJ+HBF/ERE3\n1fV8ta3WUyPi3yPi6Ii4Fbiunv+Eur876/W+GRE7dOjjlRHx03p8LoiIrVqWbRcR59Sv556IuDwi\n2r9qHGDTiDgrIn4fEbdExLva+nj0EEyH/h8d/4jYH/gisLBlfP9vRPxdRFzVYd0rIuJDaxpHabYx\ngEgzayWwfv04gfnA+4G3AX8I3B4Rfwp8CPgA1deTHw0cGxF/BtWbPnAesAwYqtt+okNfrYFkd+Ci\nep19gEX1NtYF3gMsAT5P9W2kWwI3R8RCqsA0VvdzEPAU4GstfXwCeB7wCuBFwPOB4SmMww7A66m+\nvv4gqgD12bY2BwLPBF4IvLyed3pdy8vr1xHANyNi3Zb1NqYaszcBzwWeAIy2LN8EOB84gOrryL8F\nnNsaUmrvA35ct/kH4KQJgspExsf/UqqvO7+bx8b3E1ShZOeIeHS8ImIPYBfg1C76kWaHzHRyciow\nUb2JnN3y/IVUAeQf6udvBh4Bdmlb7wbgkLZ5fwv8oH78F1S799dvWf6X9baeXT/fv36+af38y8Al\nk9T6PeCEDn1+q23eVlRfhb4D1Rv9/cBrWpY/Ebi3fVtt2zgGeJDqHInxeQdRHZp6SsvY/RpYr6XN\nDnXfe7fMe1Ld32vbxvQ5LW12rNd7ziQ1/QR4V8vzG4Hz29qMAt9oeb4KOLh+vE39fKLxfzNwZ4d+\nzwc+0/L808DFM/1v18lpOib3gEhlvaLezX8/1ZvNV4APtyx/MDOXjT+p925sD/xrvd49EXEP8EFg\nu7rZTsBVmflgy3aWrKGO3an2ZnRjN+CAtjqupfpkv309zQMuH18hM+8Crp/Ctm/KzNtani+h2huz\nY8u8n2R1LsW4nalCSmt/d9b97dzS7mGqvTbjba4HfjfeJiI2johP1Ie27qpf107A1m01to/pkrZ+\neuHzwEhErB8R84AR4F973IfUFzwJVSrru8BhVG+cv87MVW3LV7Y936T++XZa3mhrj9Q/m5zQ2N7P\nVGwCnEt1iCjalt1GdXiEBrV0km0/odqz0aq9htb5q9WQmZ1qGp/3SarDO38D/JxqbP6Nxw6NTaXO\nXjkPeAB4NdW/kfWAs3vch9QX3AMilXVvZt6Ymbd0CB+Pk9WVFbcC22fmL9qmX9XNrgF2i4jWN8x9\n17Dpq6jedCfyINUeiFZLqc5L+VWHWlYCP6Pa27DP+AoR8UQeCyaT2Toitmh5/lyqgPXTSda5huoN\neu+W/v6g7u+alnbrRcRzWtrsSHUeyLUtfZ2Wmedm5tVUh7O27dDfPh2eXzdJfZPpNL5k5iPAGcCh\nwFuBr2Tm/Q37kPqaAUTqfx8CPhARfx0Rz6ivRHlLRCyul59F9Un8CxGxc0S8lOrTfLvWPQYfBfaM\niM9GxK4RsVNEHBYRT6qX/xLYO6obao1f5fJZqnMsvhIRz6mvHjkoIr4YEZGZ91IdLvh4RLwgIsZP\nnnyENXsAOD0inh0RzwNOAr6aj7+09VGZ+TOqPTKfr6/o2Q04E7i5nj/uYeCforpSaIjqZM9LM3P8\nsMwNwGsiYrd6G1+m896VRRHxvvp3cDjwOuBTU3ht41q3+Utgk4g4ICL+ICI2aln2BaoTYg+qa5UG\nkgFE6nOZ+a9Uh2DeSrXn4j+pTmL8Rb38XqqrTnah2ktxHNVhksdtqmWbN1BdpfJs4EdUN8o6mOrN\nGqqrMh6h2pNwe0RsXZ+jsYjq78aFdS0nAHe1HOI4EvgvqgDw7frxo+dfTOIGqkMN3wQuAK4ADp/C\nem+pt39e/RpWAS+r9ySMu5fqPitnAT8A7gHe2LL8vcBd9fr/Ufe/tK2fpDpU8xyqK2GOBhZn5kVt\nbdrX6fg8M5cAnwO+SrXH5ciWZT+julLm+sz87wlfuTTLRedDo5JURkQcA7wyM4dmupZ+ERE3UF0N\nc9JM1yJNF09ClaQ+ERGbUV35sjlw2sxWI00vA4gk9Y/bgTuAd2TmipkuRppOHoKRJEnFeRKqJEkq\nzgAiSZKKM4BIkqTiDCCSJKk4A4gkSSrOACJJkoozgEiSpOIMIJIkqTgDiCRJKu7/A/84KG1hPn1n\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bdceef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification on the testing data\n",
    "y_pred = BrCaLR.predict(X_test)\n",
    "y_prob = BrCaLR.predict_proba(X_test)\n",
    "plt.plot(y_prob[:,1],y_pred,'b.')\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if $Pr(Y=1)$ is greater than 0.5, then it is classified as $Y=1$.\n",
    "\n",
    "Finally, let's quantitatively examine the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   7]\n",
      " [ 10  74]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Benign       0.93      0.95      0.94       142\n",
      "  Malignant       0.91      0.88      0.90        84\n",
      "\n",
      "avg / total       0.92      0.92      0.92       226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred, target_names=targetNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just for a comparison, here is the classification report on the same data set, by an LDA.\n",
    "\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "     Benign       0.93      0.96      0.95       142\n",
    "  Malignant       0.94      0.88      0.91        84\n",
    "\n",
    "avg / total       0.93      0.93      0.93       226\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: cryotherapy data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "This is a data set available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+). The data set (available in a CSV file **`Cryotherapy.csv`**) contains the outcomes from a cryotherapy of warts from 90 patients. These are the variables:\n",
    "\n",
    "  * **`Sex`**: Gender, 1=male, 2=female\n",
    "  * **`Age`**: Age\n",
    "  * **`Time`**: Time elapsed before treatment (months)\n",
    "  * **`NumWarts`**: Number of warts\n",
    "  * **`Type`**: Types of warts, 1=common, 2=plantar, 3=both\n",
    "  * **`Area`**: Surface area of warts (mm<sup>2</sup>)\n",
    "  * **`Success`**: Success in a cryotherapy, 1=yes, 0=no\n",
    "\n",
    "So, let's load the data set first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<CryoLogistic.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# loading the data\n",
    "CryoData = pd.read_csv('Cryotherapy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if any of the features differ between the successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sex        Age       Time  NumWarts      Type        Area\n",
      "Success                                                               \n",
      "0        1.52381  36.309524  10.035714  5.214286  2.166667  112.285714\n",
      "1        1.43750  21.854167   5.593750  5.770833  1.291667   62.687500\n"
     ]
    }
   ],
   "source": [
    "# examining the outcome vs other variables\n",
    "print(CryoData.groupby('Success').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, some features may be highly associated with a success / failure of a cryotherapy. \n",
    "\n",
    "Before I fit a logistic regression model, some categorical features need to be converted to a collection of binary variables (a.k.a., dummy variables). This is because a unit increase or decrease in a certain feature (e.g., `Type`) does not make sense. It is common, in a regression model with a categorical predictor, to set one category as the reference category. All the other categories are identified by a collection of dummy variables, each corresponding to a particular category. For example, in our case:\n",
    "\n",
    "  * **`Female`**: Binary variable, 1=females, 0=males\n",
    "      * Here, the category *males* is used as the reference category (no dummy variable).  \n",
    "  * **`Plantar`**: Binary variable, 1=plantar, 0=otherwise\n",
    "  * **`Both`**: Binary variable, 1=both, 0=otherwise\n",
    "      * Here, the category *common* is used as the reference category (no dummy variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating dummy variables for categorical variables\n",
    "CryoData['Female'] = (CryoData.Sex==2).astype(int)\n",
    "CryoData['Plantar'] = (CryoData.Type==2).astype(int)\n",
    "CryoData['Both'] = (CryoData.Type==3).astype(int)\n",
    "\n",
    "# Data for logistic regression\n",
    "CryoFeatures = np.array(CryoData.loc[:,['Age', 'Time', 'NumWarts', 'Area',\n",
    "                                        'Female', 'Plantar', 'Both']])\n",
    "CryoTargets = np.array(CryoData.loc[:,'Success'])\n",
    "featureNames = ['Age', 'Time', 'NumWarts', 'Area', 'Female', 'Plantar', 'Both']\n",
    "targetNames = ['Failure','Success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Learning from the cryotherapy data**. Split the data set into the training data and testing data sets, with the testing data set including 30% of all available observations. Then fit the logistic regression model on the training data set. Print out odds ratios associated with the features used in this analysis. Which features seem to be associated with the outcome?\n",
    "2. **Classification on the cryotherapy data**. Fit the testing data set to the logistic regression model produced earlier. Produce the confusion matrix as well as the classification report. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
